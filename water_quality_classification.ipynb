{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Quality Classification Project\n",
    "\n",
    "This notebook implements a machine learning pipeline for classifying water quality based on various parameters. The classification follows the Central Pollution Control Board (CPCB) standards for Water Quality Index (WQI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data manipulation and analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# XGBoost (try-except in case it's not available)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    xgboost_available = True\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"XGBoost not available. Will use GradientBoostingClassifier instead.\")\n",
    "\n",
    "# SMOTE for handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SHAP for model interpretability (optional)\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "except ImportError:\n",
    "    shap_available = False\n",
    "    print(\"SHAP not available. Will skip SHAP explanations.\")\n",
    "\n",
    "# For saving models\n",
    "import pickle\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('water_quality_with_final_wqi.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset Preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect WQI column if named differently\n",
    "wqi_column = None\n",
    "possible_wqi_columns = ['WQI', 'Water_Quality_Index', 'water_quality_index', 'wqi']\n",
    "\n",
    "for col in possible_wqi_columns:\n",
    "    if col in df.columns:\n",
    "        wqi_column = col\n",
    "        break\n",
    "\n",
    "if wqi_column is None:\n",
    "    raise ValueError(\"Could not find WQI column in the dataset\")\n",
    "else:\n",
    "    print(f\"WQI column identified as: {wqi_column}\")\n",
    "\n",
    "# Plot WQI distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[wqi_column].hist(bins=20, density=True, alpha=0.7, color='skyblue')\n",
    "df[wqi_column].plot(kind='kde', color='red', linewidth=2)\n",
    "plt.title(f'{wqi_column} Distribution')\n",
    "plt.xlabel(wqi_column)\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Drop duplicates if any\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Dropped {duplicate_count} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median for numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "print(\"Imputing missing values with median for numeric columns...\")\n",
    "for col in numeric_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_value = df[col].median()\n",
    "        df[col].fillna(median_value, inplace=True)\n",
    "        print(f\"  - Imputed {col} with median value: {median_value:.4f}\")\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "remaining_missing = df.isnull().sum().sum()\n",
    "print(f\"\\nRemaining missing values after imputation: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for categorical columns and encode them if any\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns: {list(categorical_columns)}\")\n",
    "\n",
    "# Initialize a dictionary to store label encoders\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode categorical columns if any\n",
    "for col in categorical_columns:\n",
    "    if col != wqi_column:  # Skip WQI column if it's categorical\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoded {col} with LabelEncoder. Classes: {list(le.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and handle outliers using IQR method\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Function to winsorize outliers\n",
    "def winsorize_outliers(df, column, lower_bound, upper_bound):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.loc[df_copy[column] < lower_bound, column] = lower_bound\n",
    "    df_copy.loc[df_copy[column] > upper_bound, column] = upper_bound\n",
    "    return df_copy\n",
    "\n",
    "# Check for outliers in numeric columns (excluding WQI column)\n",
    "numeric_columns_excl_wqi = [col for col in numeric_columns if col != wqi_column]\n",
    "\n",
    "# Store original data for comparison\n",
    "df_before_outlier_treatment = df.copy()\n",
    "\n",
    "# Detect and handle outliers\n",
    "for col in numeric_columns_excl_wqi:\n",
    "    outliers, lower_bound, upper_bound = detect_outliers_iqr(df, col)\n",
    "    outlier_percentage = (len(outliers) / len(df)) * 100\n",
    "    \n",
    "    print(f\"Column {col}: {len(outliers)} outliers detected ({outlier_percentage:.2f}%)\")\n",
    "    print(f\"  - Lower bound: {lower_bound:.4f}, Upper bound: {upper_bound:.4f}\")\n",
    "    \n",
    "    # Winsorize outliers\n",
    "    df = winsorize_outliers(df, col, lower_bound, upper_bound)\n",
    "    \n",
    "    # Plot before and after outlier treatment\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=df_before_outlier_treatment[col], color='skyblue')\n",
    "    plt.title(f'{col} - Before Outlier Treatment')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df[col], color='lightgreen')\n",
    "    plt.title(f'{col} - After Outlier Treatment (Winsorized)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WQI_Label based on CPCB standards\n",
    "def assign_wqi_label(wqi):\n",
    "    if wqi <= 25:\n",
    "        return 'Very Poor'\n",
    "    elif wqi <= 50:\n",
    "        return 'Poor'\n",
    "    elif wqi <= 70:\n",
    "        return 'Moderate'\n",
    "    elif wqi <= 90:\n",
    "        return 'Good'\n",
    "    else:\n",
    "        return 'Excellent'\n",
    "\n",
    "# Create WQI_Label column\n",
    "df['WQI_Label'] = df[wqi_column].apply(assign_wqi_label)\n",
    "\n",
    "# Create a numeric version of WQI_Label for modeling\n",
    "label_mapping = {\n",
    "    'Very Poor': 0,\n",
    "    'Poor': 1,\n",
    "    'Moderate': 2,\n",
    "    'Good': 3,\n",
    "    'Excellent': 4\n",
    "}\n",
    "\n",
    "df['WQI_Label_Numeric'] = df['WQI_Label'].map(label_mapping)\n",
    "\n",
    "# Show the counts of each class\n",
    "wqi_label_counts = df['WQI_Label'].value_counts().sort_index()\n",
    "print(\"WQI Label Counts:\")\n",
    "print(wqi_label_counts)\n",
    "\n",
    "# Plot the distribution of WQI labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.countplot(x='WQI_Label', data=df, palette='viridis', order=['Very Poor', 'Poor', 'Moderate', 'Good', 'Excellent'])\n",
    "plt.title('Distribution of Water Quality Classes (CPCB Standards)')\n",
    "plt.xlabel('Water Quality Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'bottom', \n",
    "                xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis - histograms for numeric features\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(numeric_columns_excl_wqi, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(df[col], kde=True, color='skyblue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for numeric features by WQI_Label\n",
    "for col in numeric_columns_excl_wqi:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.boxplot(x='WQI_Label', y=col, data=df, palette='viridis', \n",
    "                order=['Very Poor', 'Poor', 'Moderate', 'Good', 'Excellent'])\n",
    "    plt.title(f'Distribution of {col} by Water Quality Class')\n",
    "    plt.xlabel('Water Quality Class')\n",
    "    plt.ylabel(col)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df.drop('WQI_Label', axis=1).corr()\n",
    "mask = np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', mask=mask, linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Water Quality Parameters')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot WQI distribution with vertical lines at class boundaries\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df[wqi_column], bins=30, kde=True, color='skyblue')\n",
    "\n",
    "# Add vertical lines at class boundaries\n",
    "boundaries = [25, 50, 70, 90]\n",
    "labels = ['Very Poor/Poor', 'Poor/Moderate', 'Moderate/Good', 'Good/Excellent']\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "\n",
    "for boundary, label, color in zip(boundaries, labels, colors):\n",
    "    plt.axvline(x=boundary, color=color, linestyle='--', linewidth=2, label=label)\n",
    "\n",
    "plt.title(f'Distribution of {wqi_column} with CPCB Class Boundaries')\n",
    "plt.xlabel(wqi_column)\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection and Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop([wqi_column, 'WQI_Label', 'WQI_Label_Numeric'], axis=1)\n",
    "y = df['WQI_Label_Numeric']  # Using numeric labels for modeling\n",
    "\n",
    "# Display feature set\n",
    "print(\"Features used for modeling:\")\n",
    "print(X.columns.tolist())\n",
    "print(f\"\\nShape of feature set: {X.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split with stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(\"Standardized features (first 5 rows):\")\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in training set\n",
    "train_class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "print(\"Class distribution in training set before SMOTE:\")\n",
    "print(train_class_counts)\n",
    "\n",
    "# Calculate class imbalance ratio (max count / min count)\n",
    "imbalance_ratio = train_class_counts.max() / train_class_counts.min()\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "\n",
    "# Apply SMOTE if there's significant class imbalance (ratio > 1.5)\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"\\nApplying SMOTE to balance the training data...\")\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    # Check class distribution after SMOTE\n",
    "    resampled_class_counts = pd.Series(y_train_resampled).value_counts().sort_index()\n",
    "    print(\"\\nClass distribution in training set after SMOTE:\")\n",
    "    print(resampled_class_counts)\n",
    "    \n",
    "    # Plot before and after SMOTE\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=y_train, palette='viridis')\n",
    "    plt.title('Class Distribution Before SMOTE')\n",
    "    plt.xlabel('Water Quality Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(x=y_train_resampled, palette='viridis')\n",
    "    plt.title('Class Distribution After SMOTE')\n",
    "    plt.xlabel('Water Quality Class')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Use the resampled data for training\n",
    "    X_train_scaled = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "else:\n",
    "    print(\"\\nClass imbalance is not significant. Skipping SMOTE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'SVC': SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if xgboost_available:\n",
    "    models['XGBoost'] = XGBClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=['Very Poor', 'Poor', 'Moderate'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (macro): {precision:.4f}\")\n",
    "    print(f\"Recall (macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (macro): {f1:.4f}\")\n",
    "    print(f\"Cross-validation F1 (5-fold): {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Very Poor', 'Poor', 'Moderate'],\n",
    "                yticklabels=['Very Poor', 'Poor', 'Moderate'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'CV F1 (mean)': cv_mean,\n",
    "        'CV F1 (std)': cv_std,\n",
    "        'Model Object': model\n",
    "    }\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    result = evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, name)\n",
    "    results.append(result)\n",
    "\n",
    "# Create a DataFrame with results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display sorted results\n",
    "print(\"\\nModel Performance Summary (sorted by F1 Score):\")\n",
    "display_cols = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'CV F1 (mean)', 'CV F1 (std)']\n",
    "results_df[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning for Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 models for hyperparameter tuning\n",
    "top_models = results_df.iloc[:2]\n",
    "print(f\"Top 2 models for hyperparameter tuning:\\n{top_models[['Model', 'F1 Score']]}\")\n",
    "\n",
    "# Define hyperparameter grids for different models\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'lbfgs'],\n",
    "        'max_iter': [1000, 2000]\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tune hyperparameters for top 2 models\n",
    "tuned_models = []\n",
    "\n",
    "for _, row in top_models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    model = row['Model Object']\n",
    "    param_grid = param_grids[model_name]\n",
    "    \n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Tuning hyperparameters for {model_name}...\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of parameter settings sampled\n",
    "        cv=5,\n",
    "        scoring='f1_macro',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the random search\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get the best parameters and score\n",
    "    best_params = random_search.best_params_\n",
    "    best_score = random_search.best_score_\n",
    "    \n",
    "    print(f\"\\nBest parameters for {model_name}:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  - {param}: {value}\")\n",
    "    print(f\"\\nBest cross-validation F1 score: {best_score:.4f}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the tuned model\n",
    "    tuned_result = evaluate_model(best_model, X_train_scaled, y_train, X_test_scaled, y_test, f\"{model_name} (Tuned)\")\n",
    "    tuned_models.append(tuned_result)\n",
    "\n",
    "# Add tuned models to results DataFrame\n",
    "tuned_results_df = pd.DataFrame(tuned_models)\n",
    "results_df = pd.concat([results_df, tuned_results_df], ignore_index=True)\n",
    "results_df = results_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display updated results\n",
    "print(\"\\nUpdated Model Performance Summary (including tuned models):\")\n",
    "results_df[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model_row = results_df.iloc[0]\n",
    "best_model_name = best_model_row['Model']\n",
    "best_model = best_model_row['Model Object']\n",
    "\n",
    "print(f\"Best model: {best_model_name} with F1 Score: {best_model_row['F1 Score']:.4f}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"\\nDetailed Evaluation of Best Model:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Very Poor', 'Poor', 'Moderate']))\n",
    "\n",
    "# Confusion matrix with percentages\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Very Poor', 'Poor', 'Moderate'],\n",
    "            yticklabels=['Very Poor', 'Poor', 'Moderate'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Percentage confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', \n",
    "            xticklabels=['Very Poor', 'Poor', 'Moderate'],\n",
    "            yticklabels=['Very Poor', 'Poor', 'Moderate'])\n",
    "plt.title(f'Confusion Matrix (%) - {best_model_name}')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = None\n",
    "\n",
    "# Extract feature importance based on model type\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importance = best_model.feature_importances_\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    feature_importance = np.abs(best_model.coef_).mean(axis=0) if best_model.coef_.ndim > 1 else np.abs(best_model.coef_)\n",
    "\n",
    "if feature_importance is not None:\n",
    "    # Create DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importance\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display feature importance\n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nFeature importance not available for this model type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis (if available)\n",
    "if shap_available:\n",
    "    try:\n",
    "        print(\"\\nGenerating SHAP explanations...\")\n",
    "        \n",
    "        # Sample a subset of test data for SHAP analysis (for efficiency)\n",
    "        X_test_sample = X_test_scaled[:100]\n",
    "        \n",
    "        # Create explainer based on model type\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            explainer = shap.Explainer(best_model, X_train_scaled)\n",
    "            shap_values = explainer(X_test_sample)\n",
    "            \n",
    "            # Summary plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.summary_plot(shap_values, X_test_sample, feature_names=X.columns, show=False)\n",
    "            plt.title(f'SHAP Summary Plot - {best_model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Detailed SHAP values for a few examples\n",
    "            for i in range(min(5, len(X_test_sample))):\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                shap.plots.waterfall(shap_values[i], show=False)\n",
    "                plt.title(f'SHAP Explanation for Example {i+1} (True Class: {y_test.iloc[i]})')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"SHAP analysis not compatible with this model type.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in SHAP analysis: {e}\")\n",
    "else:\n",
    "    print(\"\\nSHAP not available. Skipping SHAP explanations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "with open('best_water_quality_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"Best model ({best_model_name}) saved as 'best_water_quality_model.pkl'\")\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save label mapping\n",
    "with open('label_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "print(\"Label mapping saved as 'label_mapping.pkl'\")\n",
    "\n",
    "# Save feature names\n",
    "with open('feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(X.columns.tolist(), f)\n",
    "print(\"Feature names saved as 'feature_names.pkl'\")\n",
    "\n",
    "# Save any label encoders if used\n",
    "if label_encoders:\n",
    "    with open('label_encoders.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoders, f)\n",
    "    print(\"Label encoders saved as 'label_encoders.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create Demo Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demo test cases (one for each class)\n",
    "# Get representative samples from each class in the original dataset\n",
    "demo_cases = []\n",
    "\n",
    "for label in range(5):  # 0 to 4 (Very Poor to Excellent)\n",
    "    # Find samples with this label\n",
    "    class_samples = df[df['WQI_Label_Numeric'] == label]\n",
    "    \n",
    "    if len(class_samples) > 0:\n",
    "        # Take the first sample from this class\n",
    "        sample = class_samples.iloc[0]\n",
    "        \n",
    "        # Extract features (excluding WQI, WQI_Label, and WQI_Label_Numeric)\n",
    "        features = sample.drop([wqi_column, 'WQI_Label', 'WQI_Label_Numeric'])\n",
    "        \n",
    "        # Add label information for reference\n",
    "        features_dict = features.to_dict()\n",
    "        features_dict['Actual_WQI'] = sample[wqi_column]\n",
    "        features_dict['Actual_Label'] = sample['WQI_Label']\n",
    "        \n",
    "        demo_cases.append(features_dict)\n",
    "    else:\n",
    "        print(f\"Warning: No samples found for class {label}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "demo_df = pd.DataFrame(demo_cases)\n",
    "\n",
    "# Display demo cases\n",
    "print(\"Demo Test Cases:\")\n",
    "demo_df\n",
    "\n",
    "# Save demo cases to CSV\n",
    "demo_df.to_csv('demo_test_cases.csv', index=False)\n",
    "print(\"Demo test cases saved as 'demo_test_cases.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of the project\n",
    "print(\"Water Quality Classification Project Summary\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Dataset size: {len(df)} samples\")\n",
    "print(f\"Features used: {X.shape[1]}\")\n",
    "print(f\"Class distribution: {df['WQI_Label'].value_counts().to_dict()}\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Best model F1 score: {best_model_row['F1 Score']:.4f}\")\n",
    "print(f\"Best model accuracy: {best_model_row['Accuracy']:.4f}\")\n",
    "print(\"\\nModel and components saved for use in the Streamlit application.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
